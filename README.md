# automated-interpretability-mistral
Getting OpenAI autointerp to work with locally-run (finetuned) Mistral-7B instances.

## Hopeful eventual goals
- [ ] `NeuronExplainer` instances for local `llama-cpp-python` or HuggingFace `transformers` models.
- [ ] Finetuned Mistral-7B-Chat models for autointerpretation.
- [ ] Evaluation of said autointerpretation models.
